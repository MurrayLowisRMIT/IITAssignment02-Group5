<!DOCTYPE html>
<html>
<head>
	<link rel="stylesheet" href="styles.css">
	<div class="ITTechnologies">
	<body class="iframe">
</head>
<body>

	<div class="contents">
		<h1>Natural Language Processing</h1>
		
		<h2>What is Natural Language Processing?</h2>
		<p>Natural language processing, or NLP, is essentially the understanding of natural language to computers and the ability to generate data into natural understandable language. It is a very complicated aspect of technology that effectively blends computer science with linguistics. It has a storied history of continual and systematic developments that have led to the robust position it now holds in the technology industry. It is used in speech recognition, answering questions, chatterbots, machine translation and the list goes on. Most high-profile search engines, operating systems and a multitude of different programs use several different forms of natural language processing. With the rise of voice user interfaces such as Cortana, Siri or Alexa speech recognition is becoming the fastest and most convenient method of retrieving information or even giving directives to your mobile phone, computer, gaming console or car. An example of this convenience would be the ease and simplicity of commanding your mobile phone to ‘set an alarm for 9.30am’ rather than having to unlock your phone, open the application and manually set the alarm. It has also made phone calls while driving a possibility since modern cars are often equipped with Bluetooth connection and sometimes even speech recognition software of their own. As developments in natural language processing and in computing power have propelled it to new heights in the last couple decades its utility and benefits are unquestionable. Some form or another of natural language processing is ubiquitous with almost any usage of technology today. Whether that is browsing the internet, using a mobile phone or even operating household technology such as lights, televisions, or automatic blinds.
		</p>

		<h2>How Natural Language Processing Works</h2>
		<p>Natural language processing is made up of two main components. One being natural language understanding and the other being natural language generation. Natural language understanding governs the ability for software to understand any given text or speech and interpret it, whereas natural language generation is the ability to construct natural language. For instance, the English language has 9 fundamental types of words which are nouns, verbs, pronouns, adjectives, adverbs, conjunctions, interjections, articles and prepositions. NLP software breaks down the entire English language into understandable sections and usable words to create natural language. The main way it achieves this is by executing a few processes such as tokenisation, stemming, lemmatisation, POS tags, named entity recognition and chunking. Tokenisation is the ability to section off parts of a language such as words or symbols to understand them individually. Stemming is the ability to originate words into their root forms to understand their meaning. Lemmatization is a somewhat more inclusive version of stemming. It can map words back to the original root word similar to stemming however it is not limited to simple conjugation or pluralisation. For example, ‘went’ would be converted into ‘go’. POS tags are tags given to words to understand their place within the language. Such as labelling a word as an adjective, noun or verb. By applying POS tags to certain kinds of words the software is able to understand the given sentence structure. Named entity recognition is how nouns are labelled to be a location, person, organisation, etc. Finally, chunks are how the individual information from the POS tags, tokenisation, stemming, lemmatization and named entity recognition all come together to form a complete and natural sentence. [34] [35]
		</p>

		<h2>Speech Recognition and Chatterbots</h2>
		<p>Natural language processing has not always been an important part of the technology industry. For example, the earliest known speech recognition software was created in 1952 and was nicknamed Audrey (Automatic Digit Recogniser). This software was able to recognise all ten numerical digits if it was said slowly enough and clearly enough for Audrey to understand. Although this may seem like a breakthrough in technology the utility of it simply was not applicable enough given that it was considerably faster to simply input the digit at the press of a button. However, Audrey gave rise to the idea of speech recognition and is the earliest precursor to the modern-day complex software that runs programs such as Alexa or Siri.
		</p><p>Next in line of significance in the breadth of technological advancements of speech recognition was a program called HARPY which was the first system to recognise over 1,000 words. [36] Regardless of the large diction it possessed, mainstream use of the advancement was extremely limited due to the large amount of processing time. Computers of its time were simply not powerful enough to transcribe natural language fast enough for HARPY to be an effective addition to programs or technology. With advancements in computing power came the feasibility of speech recognition software and is the main reason for its emancipation and ubiquitous usage today. [34]
		</p><p>However, speech recognition is just one application of natural language processing. Chatterbots are another usage of NLP which are found within the realm of customer service frequently. It also pairs very well with social media. Open Universities which I happen to study through also have their own chatterbot which I have found useful in the past. Chatterbots have also served to make some types of customer service or helpdesk roles redundant as a chatterbot dispensing advice on the most common troubleshooting issues significantly cuts down the amount of inquiries the helpdesk employee is likely to receive. The oldest forms of chatterbots would rely on programmers to individually map out what a user could say and in turn map out an appropriate response. This was extremely difficult to create, maintain or update and would not account for spelling errors or most importantly other languages. As it was such a taxing application of natural language processing it did not find a strong footing until very large sample sizes of human to human interaction data was available. This allowed artificial intelligence to spear head and streamline the process which could account for multiple languages, Large diverse vocabularies, words with multiple meanings, unique word play, expressions or slang. This has all led to chatterbots being so believable that they can have a complete conversation with someone and be a convincing substitute for human interaction.

		<h2>Conclusion</h2>
		<p>Personally, natural language processing plays a few roles in my own life. When I google search anything, I rarely type the entire sentence. When I watch a YouTube clip, I rarely watch without automated captions. When I text someone, I heavily rely on predictive text not to embarrass myself. And when I browse Facebook, I often rely on the machine translations to understand friends of a different language. 
		</p><p>Natural language processing occupies such a considerable section in technology that its effects are almost impossible to count and as the bridge between computer and natural language gets smaller and smaller the utility for it will get more and more ingrained in society. From autonomous cars to robots and maybe even controllerless gaming consoles. The future of technology looks bright and the future of natural language processing looks even brighter.
		</p>
	</div>
</body>
</html>